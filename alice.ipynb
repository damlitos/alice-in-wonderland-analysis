{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "051cdfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ArifogluD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import Counter\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a0debe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove punctuation characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove unusual characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "   \n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    # Stem the words\n",
    "    \n",
    "    words = text.split()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text) \n",
    "    \n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd41798",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "eae68c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chapters = dict(zip(range(1, 13), ['' for i in range(13)]))\n",
    "\n",
    "url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "text = requests.get(url).text\n",
    "\n",
    "texts = text.split(\"CHAPTER\")[13:] #  skip part before CHAPTER I\n",
    "\n",
    "#  put every chapter into chapters dictionary item\n",
    "for i, chapter in enumerate(texts):\n",
    "    chapter_id = i + 1\n",
    "    chapter_text = chapter.split(\"\\n\", 1)[1].strip()\n",
    "    chapters[chapter_id] = clean_text(chapter_text)\n",
    "\n",
    "#  now we have every chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a5d6f319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      chapter_number       word  count\n",
      "0                  1  rabbithol      3\n",
      "1                  1       alic     28\n",
      "2                  1      begin      2\n",
      "3                  1        get     10\n",
      "4                  1       tire      2\n",
      "...              ...        ...    ...\n",
      "6088              12         pg      1\n",
      "6089              12     search      1\n",
      "6090              12      facil      1\n",
      "6091              12   subscrib      1\n",
      "6092              12   newslett      1\n",
      "\n",
      "[6093 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a list of lists to store the word count data\n",
    "word_counts = []\n",
    "for chapter_id, chapter_text in chapters.items():\n",
    "    words = chapter_text.split()\n",
    "    word_count = Counter(words)\n",
    "    for word, count in word_count.items():\n",
    "        word_counts.append([chapter_id, word, count])\n",
    "        \n",
    "# Create a DataFrame from the word count data\n",
    "df = pd.DataFrame(word_counts, columns=[\"chapter_number\", \"word\", \"count\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "\n",
    "# Write the word count data to a CSV file with UTF-8 encoding\n",
    "with open(\"word_counts.csv\", \"w\", newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"chapter_number\", \"word\", \"count\"])\n",
    "    writer.writerows(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2ed1865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.26.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "913b6422",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15844\\2140076968.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msummarizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"summarization\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"facebook/bart-large-cnn\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Input text to be summarized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\pipelines\\__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;31m# Will load the correct model if possible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m     \u001b[0mmodel_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m     framework, model = infer_framework_load_model(\n\u001b[0m\u001b[0;32m    755\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[0mmodel_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py\u001b[0m in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \"\"\"\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         raise RuntimeError(\n\u001b[0m\u001b[0;32m    210\u001b[0m             \u001b[1;34m\"At least one of TensorFlow 2.0 or PyTorch should be installed. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;34m\"To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Input text to be summarized\n",
    "text = chapters[0]\n",
    "\n",
    "# Generate a summary of the desired length\n",
    "summary_length = 30\n",
    "output = summarizer(text, max_length=summary_length, min_length=30, length_penalty=2.0, num_beams=4)\n",
    "\n",
    "# Extract the generated summary\n",
    "summary = output[0]['summary_text']\n",
    "\n",
    "# Print the summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e327b3f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bertopic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15844\\1943223217.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbertopic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtopic_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtopics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bertopic'"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cd6cf705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers[tf-cpu]\n",
      "  Using cached transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from transformers[tf-cpu]) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from transformers[tf-cpu]) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from transformers[tf-cpu]) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from transformers[tf-cpu]) (0.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from transformers[tf-cpu]) (2022.7.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from transformers[tf-cpu]) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from transformers[tf-cpu]) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from transformers[tf-cpu]) (1.21.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from transformers[tf-cpu]) (3.6.0)\n",
      "Collecting tf2onnx\n",
      "  Downloading tf2onnx-1.13.0-py3-none-any.whl (442 kB)\n",
      "     -------------------------------------- 442.3/442.3 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting tensorflow-cpu<2.12,>=2.4\n",
      "  Downloading tensorflow_cpu-2.11.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting onnxconverter-common\n",
      "  Downloading onnxconverter_common-1.13.0-py2.py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 83.8/83.8 kB 4.9 MB/s eta 0:00:00\n",
      "Collecting keras-nlp>=0.3.1\n",
      "  Downloading keras_nlp-0.4.0-py3-none-any.whl (337 kB)\n",
      "     -------------------------------------- 337.5/337.5 kB 7.1 MB/s eta 0:00:00\n",
      "Collecting tensorflow-text\n",
      "  Downloading tensorflow_text-2.10.0-cp39-cp39-win_amd64.whl (5.0 MB)\n",
      "     ---------------------------------------- 5.0/5.0 MB 6.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers[tf-cpu]) (4.3.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from keras-nlp>=0.3.1->transformers[tf-cpu]) (1.4.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from keras-nlp>=0.3.1->transformers[tf-cpu]) (2.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers[tf-cpu]) (3.0.9)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (2.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (63.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (3.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (3.7.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (2.11.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (2.11.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (0.30.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (1.51.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (15.0.6.1)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (2.11.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (0.4.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (3.19.6)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (2.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow-cpu<2.12,>=2.4->transformers[tf-cpu]) (23.1.21)\n",
      "Requirement already satisfied: colorama in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[tf-cpu]) (0.4.5)\n",
      "Collecting onnx\n",
      "  Downloading onnx-1.13.0-cp39-cp39-win_amd64.whl (12.2 MB)\n",
      "     ---------------------------------------- 12.2/12.2 MB 7.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from requests->transformers[tf-cpu]) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from requests->transformers[tf-cpu]) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from requests->transformers[tf-cpu]) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arifoglud\\anaconda3\\lib\\site-packages (from requests->transformers[tf-cpu]) (3.3)\n",
      "Collecting tensorflow-hub>=0.8.0\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "     -------------------------------------- 108.8/108.8 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.10.1-cp39-cp39-win_amd64.whl (455.9 MB)\n",
      "     ---------------------------------      401.8/455.9 MB 3.0 MB/s eta 0:00:18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 435, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 516, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 90, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\http\\client.py\", line 463, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\http\\client.py\", line 507, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\ssl.py\", line 1242, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\ssl.py\", line 1100, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 167, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 247, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 369, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 92, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 481, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 373, in resolve\n",
      "    failure_causes = self._attempt_to_pin_criterion(name)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 213, in _attempt_to_pin_criterion\n",
      "    criteria = self._get_updated_criteria(candidate)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 204, in _get_updated_criteria\n",
      "    self._add_to_criteria(criteria, requirement, parent=candidate)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 172, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 151, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 155, in __bool__\n",
      "    return any(self)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 143, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 47, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 206, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 297, in __init__\n",
      "    super().__init__(\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 162, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 231, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 308, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 438, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 483, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 165, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 106, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 147, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 53, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 573, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 538, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\contextlib.py\", line 137, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"C:\\Users\\ArifogluD\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 452, in _error_catcher\n",
      "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
      "pip._vendor.urllib3.exceptions.ProtocolError: (\"Connection broken: ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)\", ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[tf-cpu]\n",
    "\n",
    "# import transformers\n",
    "\n",
    "# # Load the BERTopic model\n",
    "# model = transformers.BERTopic.from_pretrained(\"dmis-lab/bert-topic-news\")\n",
    "\n",
    "# # Input text for topic modeling\n",
    "# text = \"This text can be about any topic, for example about natural language processing or machine learning.\"\n",
    "\n",
    "# # Encode the input text with the BERTopic model\n",
    "# input_ids = model.encode(text)\n",
    "\n",
    "# # Generate the topic distributions\n",
    "# topic_distributions = model.predict(input_ids)\n",
    "\n",
    "# # Extract the top 5 topics\n",
    "# top_topics = topic_distributions.argsort()[:, -5:][0]\n",
    "\n",
    "# # Print the top 5 topics\n",
    "# print(top_topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
